{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Question 2: Mountain Car with Q-Learning\n",
        "Dataset Problem: Use OpenAI Gym's MountainCar-v0 environment to train a Q-learning agent.\n",
        "Similar to the CartPole example, but with the Mountain Car environment. The Q-learning code will be similar, with adjustments to the state and action space to fit the Mountain Car environment.\n"
      ],
      "metadata": {
        "id": "VJfUTV612xle"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this implementation, I use a very simple network with the following structure:"
      ],
      "metadata": {
        "id": "Ou-cU9iO_G57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade gym numpy"
      ],
      "metadata": {
        "id": "yBmnqoIvO70z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.23.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfqAvZQVPAcK",
        "outputId": "152efa82-90c7-45e0-d044-b6e935c86859"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.11/dist-packages (1.23.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "# Initialize environment\n",
        "env = gym.make(\"MountainCar-v0\")\n",
        "\n",
        "# Hyperparameters\n",
        "num_episodes = 10000\n",
        "alpha = 0.1  # Learning rate\n",
        "gamma = 0.99  # Discount factor\n",
        "epsilon = 1.0  # Exploration-exploitation trade-off\n",
        "epsilon_min = 0.01\n",
        "decay_rate = 0.995\n",
        "\n",
        "# Discretization parameters\n",
        "num_bins = [20, 20]  # Binning for position and velocity\n",
        "state_bins = [\n",
        "    np.linspace(-1.2, 0.6, num_bins[0] - 1),  # Position\n",
        "    np.linspace(-0.07, 0.07, num_bins[1] - 1)  # Velocity\n",
        "]\n",
        "\n",
        "# Initialize Q-table\n",
        "q_table = np.zeros((num_bins[0], num_bins[1], env.action_space.n))\n",
        "\n",
        "def discretize_state(state):\n",
        "    \"\"\"Converts continuous state to discrete bins.\"\"\"\n",
        "    return tuple(np.digitize(state[i], state_bins[i]) - 1 for i in range(len(state)))\n",
        "\n",
        "# Training loop\n",
        "for episode in range(num_episodes):\n",
        "    state, _ = env.reset()\n",
        "    state = discretize_state(state)\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        # Epsilon-greedy action selection\n",
        "        if np.random.rand() < epsilon:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            action = np.argmax(q_table[state])\n",
        "\n",
        "        new_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        new_state = discretize_state(new_state)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        # Update Q-table\n",
        "        best_future_q = np.max(q_table[new_state])\n",
        "        q_table[state][action] += alpha * (reward + gamma * best_future_q - q_table[state][action])\n",
        "\n",
        "        state = new_state\n",
        "        total_reward += reward\n",
        "\n",
        "    # Decay epsilon\n",
        "    epsilon = max(epsilon * decay_rate, epsilon_min)\n",
        "\n",
        "    if episode % 1000 == 0:\n",
        "        print(f\"Episode {episode}, Total Reward: {total_reward}\")\n",
        "\n",
        "# Evaluation\n",
        "state, _ = env.reset()\n",
        "state = discretize_state(state)\n",
        "done = False\n",
        "total_reward = 0\n",
        "\n",
        "while not done:\n",
        "    action = np.argmax(q_table[state])\n",
        "    new_state, reward, terminated, truncated, _ = env.step(action)\n",
        "    new_state = discretize_state(new_state)\n",
        "    done = terminated or truncated\n",
        "    total_reward += reward\n",
        "    env.render()\n",
        "\n",
        "env.close()\n",
        "print(f\"Total Reward: {total_reward}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORA4ZNnAPvLM",
        "outputId": "cb57c50a-1d25-4531-ffcc-8b0da4ddcf7a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0, Total Reward: -200.0\n",
            "Episode 1000, Total Reward: -200.0\n",
            "Episode 2000, Total Reward: -153.0\n",
            "Episode 3000, Total Reward: -200.0\n",
            "Episode 4000, Total Reward: -115.0\n",
            "Episode 5000, Total Reward: -163.0\n",
            "Episode 6000, Total Reward: -144.0\n",
            "Episode 7000, Total Reward: -137.0\n",
            "Episode 8000, Total Reward: -152.0\n",
            "Episode 9000, Total Reward: -180.0\n",
            "Total Reward: -200.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/envs/classic_control/mountain_car.py:171: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"MountainCar-v0\", render_mode=\"rgb_array\")\u001b[0m\n",
            "  gym.logger.warn(\n"
          ]
        }
      ]
    }
  ]
}